
% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@MISC{Ringach2017-bb,
  title        = "Spikefinder: the Vanilla algorithm",
  booktitle    = "Scanbox",
  author       = "Ringach, Dario",
  abstract     = "Over the last few months, the Spikefinder challenge has
                  provided a playing ground for colleagues to offer ideas about
                  how to estimate the spiking of individual neurons based on
                  the measured activity of fluorescent calcium indicators. The
                  challenge was for people to come up with strategies that beat
                  the performance of state-of-the-art algorithms, STM \& oopsi.
                  A good number of algorithms were ableâ€¦",
  month        =  "15~" # may,
  year         =  2017,
  howpublished = "\url{https://scanbox.org/2017/05/15/spikefinder-the-vanilla-algorithm/}",
  note         = "Accessed: 2017-5-18"
}

@MISC{Otoro2015-db,
  title        = "Handwriting Generation Demo in {TensorFlow}",
  booktitle    = "Otoro",
  author       = "{Otoro}",
  year         =  2015,
  howpublished = "\url{http://blog.otoro.net/2015/12/12/handwriting-generation-demo-in-tensorflow/}",
  note         = "Accessed: 2017-5-18"
}

@ARTICLE{Olah2016-be,
  title   = "Attention and Augmented Recurrent Neural Networks",
  author  = "Olah, Chris and Carter, Shan",
  journal = "Distill",
  volume  =  1,
  number  =  9,
  month   =  "8~" # sep,
  year    =  2016
}

@ARTICLE{Kingma2014-if,
  title         = "Adam: A Method for Stochastic Optimization",
  author        = "Kingma, Diederik P and Ba, Jimmy",
  abstract      = "We introduce Adam, an algorithm for first-order
                   gradient-based optimization of stochastic objective
                   functions, based on adaptive estimates of lower-order
                   moments. The method is straightforward to implement, is
                   computationally efficient, has little memory requirements,
                   is invariant to diagonal rescaling of the gradients, and is
                   well suited for problems that are large in terms of data
                   and/or parameters. The method is also appropriate for
                   non-stationary objectives and problems with very noisy
                   and/or sparse gradients. The hyper-parameters have intuitive
                   interpretations and typically require little tuning. Some
                   connections to related algorithms, on which Adam was
                   inspired, are discussed. We also analyze the theoretical
                   convergence properties of the algorithm and provide a regret
                   bound on the convergence rate that is comparable to the best
                   known results under the online convex optimization
                   framework. Empirical results demonstrate that Adam works
                   well in practice and compares favorably to other stochastic
                   optimization methods. Finally, we discuss AdaMax, a variant
                   of Adam based on the infinity norm.",
  month         =  "22~" # dec,
  year          =  2014,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "1412.6980"
}

@ARTICLE{Pnevmatikakis2016-rp,
  title       = "Simultaneous Denoising, Deconvolution, and Demixing of Calcium
                 Imaging Data",
  author      = "Pnevmatikakis, Eftychios A and Soudry, Daniel and Gao, Yuanjun
                 and Machado, Timothy A and Merel, Josh and Pfau, David and
                 Reardon, Thomas and Mu, Yu and Lacefield, Clay and Yang,
                 Weijian and Ahrens, Misha and Bruno, Randy and Jessell, Thomas
                 M and Peterka, Darcy S and Yuste, Rafael and Paninski, Liam",
  affiliation = "Center for Computational Biology, Simons Foundation, New York,
                 NY 10010, USA; Department of Statistics, Center for
                 Theoretical Neuroscience, and Grossman Center for the
                 Statistics of Mind, Columbia University, New York, NY 10027,
                 USA. Electronic address: epnevmatikakis@simonsfoundation.org.
                 Department of Statistics, Center for Theoretical Neuroscience,
                 and Grossman Center for the Statistics of Mind, Columbia
                 University, New York, NY 10027, USA. Department of Statistics,
                 Center for Theoretical Neuroscience, and Grossman Center for
                 the Statistics of Mind, Columbia University, New York, NY
                 10027, USA. Department of Statistics, Center for Theoretical
                 Neuroscience, and Grossman Center for the Statistics of Mind,
                 Columbia University, New York, NY 10027, USA; Department of
                 Biochemistry and Molecular Biophysics and Howard Hughes
                 Medical Institute, Columbia University, New York, NY 10032,
                 USA; Department of Neuroscience and Kavli Institute of Brain
                 Science, Columbia University, New York, NY 10032, USA;
                 Zuckerman Mind Brain Behavior Institute, Columbia University,
                 New York, NY 10032, USA. Department of Statistics, Center for
                 Theoretical Neuroscience, and Grossman Center for the
                 Statistics of Mind, Columbia University, New York, NY 10027,
                 USA; Department of Neuroscience and Kavli Institute of Brain
                 Science, Columbia University, New York, NY 10032, USA.
                 Department of Statistics, Center for Theoretical Neuroscience,
                 and Grossman Center for the Statistics of Mind, Columbia
                 University, New York, NY 10027, USA; Department of
                 Neuroscience and Kavli Institute of Brain Science, Columbia
                 University, New York, NY 10032, USA. Department of
                 Biochemistry and Molecular Biophysics and Howard Hughes
                 Medical Institute, Columbia University, New York, NY 10032,
                 USA; Department of Neuroscience and Kavli Institute of Brain
                 Science, Columbia University, New York, NY 10032, USA;
                 Zuckerman Mind Brain Behavior Institute, Columbia University,
                 New York, NY 10032, USA. Howard Hughes Medical Institute,
                 Janelia Research Campus, Ashburn, VA 20147, USA. Department of
                 Neuroscience and Kavli Institute of Brain Science, Columbia
                 University, New York, NY 10032, USA. Neurotechnology Center,
                 Department of Biological Sciences, Columbia University, New
                 York, NY 10027, USA. Howard Hughes Medical Institute, Janelia
                 Research Campus, Ashburn, VA 20147, USA. Department of
                 Neuroscience and Kavli Institute of Brain Science, Columbia
                 University, New York, NY 10032, USA. Department of
                 Biochemistry and Molecular Biophysics and Howard Hughes
                 Medical Institute, Columbia University, New York, NY 10032,
                 USA; Department of Neuroscience and Kavli Institute of Brain
                 Science, Columbia University, New York, NY 10032, USA;
                 Zuckerman Mind Brain Behavior Institute, Columbia University,
                 New York, NY 10032, USA. Zuckerman Mind Brain Behavior
                 Institute, Columbia University, New York, NY 10032, USA;
                 Neurotechnology Center, Department of Biological Sciences,
                 Columbia University, New York, NY 10027, USA. Department of
                 Neuroscience and Kavli Institute of Brain Science, Columbia
                 University, New York, NY 10032, USA; Neurotechnology Center,
                 Department of Biological Sciences, Columbia University, New
                 York, NY 10027, USA. Department of Statistics, Center for
                 Theoretical Neuroscience, and Grossman Center for the
                 Statistics of Mind, Columbia University, New York, NY 10027,
                 USA; Department of Neuroscience and Kavli Institute of Brain
                 Science, Columbia University, New York, NY 10032, USA;
                 Zuckerman Mind Brain Behavior Institute, Columbia University,
                 New York, NY 10032, USA; Neurotechnology Center, Department of
                 Biological Sciences, Columbia University, New York, NY 10027,
                 USA. Electronic address: liam@stat.columbia.edu.",
  abstract    = "We present a modular approach for analyzing calcium imaging
                 recordings of large neuronal ensembles. Our goal is to
                 simultaneously identify the locations of the neurons, demix
                 spatially overlapping components, and denoise and deconvolve
                 the spiking activity from the slow dynamics of the calcium
                 indicator. Our approach relies on a constrained nonnegative
                 matrix factorization that expresses the spatiotemporal
                 fluorescence activity as the product of a spatial matrix that
                 encodes the spatial footprint of each neuron in the optical
                 field and a temporal matrix that characterizes the calcium
                 concentration of each neuron over time. This framework is
                 combined with a novel constrained deconvolution approach that
                 extracts estimates of neural activity from fluorescence
                 traces, to create a spatiotemporal processing algorithm that
                 requires minimal parameter tuning. We demonstrate the general
                 applicability of our method by applying it to in vitro and in
                 vivo multi-neuronal imaging data, whole-brain light-sheet
                 imaging data, and dendritic imaging data.",
  journal     = "Neuron",
  volume      =  89,
  number      =  2,
  pages       = "285--299",
  month       =  "20~" # jan,
  year        =  2016,
  language    = "en"
}

@ARTICLE{Abadi2016-me,
  title         = "{TensorFlow}: {Large-Scale} Machine Learning on
                   Heterogeneous Distributed Systems",
  author        = "Abadi, Mart{\'\i}n and Agarwal, Ashish and Barham, Paul and
                   Brevdo, Eugene and Chen, Zhifeng and Citro, Craig and
                   Corrado, Greg S and Davis, Andy and Dean, Jeffrey and Devin,
                   Matthieu and Ghemawat, Sanjay and Goodfellow, Ian and Harp,
                   Andrew and Irving, Geoffrey and Isard, Michael and Jia,
                   Yangqing and Jozefowicz, Rafal and Kaiser, Lukasz and
                   Kudlur, Manjunath and Levenberg, Josh and Mane, Dan and
                   Monga, Rajat and Moore, Sherry and Murray, Derek and Olah,
                   Chris and Schuster, Mike and Shlens, Jonathon and Steiner,
                   Benoit and Sutskever, Ilya and Talwar, Kunal and Tucker,
                   Paul and Vanhoucke, Vincent and Vasudevan, Vijay and Viegas,
                   Fernanda and Vinyals, Oriol and Warden, Pete and Wattenberg,
                   Martin and Wicke, Martin and Yu, Yuan and Zheng, Xiaoqiang",
  abstract      = "TensorFlow is an interface for expressing machine learning
                   algorithms, and an implementation for executing such
                   algorithms. A computation expressed using TensorFlow can be
                   executed with little or no change on a wide variety of
                   heterogeneous systems, ranging from mobile devices such as
                   phones and tablets up to large-scale distributed systems of
                   hundreds of machines and thousands of computational devices
                   such as GPU cards. The system is flexible and can be used to
                   express a wide variety of algorithms, including training and
                   inference algorithms for deep neural network models, and it
                   has been used for conducting research and for deploying
                   machine learning systems into production across more than a
                   dozen areas of computer science and other fields, including
                   speech recognition, computer vision, robotics, information
                   retrieval, natural language processing, geographic
                   information extraction, and computational drug discovery.
                   This paper describes the TensorFlow interface and an
                   implementation of that interface that we have built at
                   Google. The TensorFlow API and a reference implementation
                   were released as an open-source package under the Apache 2.0
                   license in November, 2015 and are available at
                   www.tensorflow.org.",
  month         =  "14~" # mar,
  year          =  2016,
  archivePrefix = "arXiv",
  primaryClass  = "cs.DC",
  eprint        = "1603.04467"
}

@UNPUBLISHED{Jimenez2017-pj,
  title    = "Untangling cortical maps in mouse primary visual cortex",
  author   = "Jimenez, Luis O and Tring, Elaine and Trachtenberg, Joshua T and
              Ringach, Dario L",
  abstract = "Local populations of neurons in mouse visual cortex exhibit
              diverse tuning preferences. We show this seeming disorder can be
              untangled -- the similarity of tuning between pairs of neurons is
              correlated better with the overlap between their receptive fields
              in visual space rather than with their distance in the cortex.
              These findings are consistent with the hypothesis that
              salt-and-pepper maps arise from the lateral dispersion of
              clonally related neurons.",
  journal  = "bioRxiv",
  pages    = "102079",
  month    =  "21~" # jan,
  year     =  2017,
  language = "en"
}

@ARTICLE{Chichilnisky2001-wd,
  title       = "A simple white noise analysis of neuronal light responses",
  author      = "Chichilnisky, E J",
  affiliation = "Systems Neurobiology, The Salk Institute, La Jolla, CA
                 92037-1099, USA. ej@salk.edu",
  abstract    = "A white noise technique is presented for estimating the
                 response properties of spiking visual system neurons. The
                 technique is simple, robust, efficient and well suited to
                 simultaneous recordings from multiple neurons. It provides a
                 complete and easily interpretable model of light responses
                 even for neurons that display a common form of response
                 nonlinearity that precludes classical linear systems analysis.
                 A theoretical justification of the technique is presented that
                 relies only on elementary linear algebra and statistics.
                 Implementation is described with examples. The technique and
                 the underlying model of neural responses are validated using
                 recordings from retinal ganglion cells, and in principle are
                 applicable to other neurons. Advantages and disadvantages of
                 the technique relative to classical approaches are discussed.",
  journal     = "Network",
  volume      =  12,
  number      =  2,
  pages       = "199--213",
  month       =  may,
  year        =  2001,
  language    = "en"
}

@ARTICLE{Srivastava2014-dc,
  title     = "Dropout: a simple way to prevent neural networks from
               overfitting",
  author    = "Srivastava, N and Hinton, G E and Krizhevsky, A and {others}",
  abstract  = "Abstract Deep neural nets with a large number of parameters are
               very powerful machine learning systems. However, overfitting is
               a serious problem in such networks. Large networks are also slow
               to use, making it difficult to deal with overfitting by
               combining the",
  journal   = "J. Mach. Learn. Res.",
  publisher = "jmlr.org",
  year      =  2014
}

@ARTICLE{He2015-hu,
  title         = "Deep Residual Learning for Image Recognition",
  author        = "He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun,
                   Jian",
  abstract      = "Deeper neural networks are more difficult to train. We
                   present a residual learning framework to ease the training
                   of networks that are substantially deeper than those used
                   previously. We explicitly reformulate the layers as learning
                   residual functions with reference to the layer inputs,
                   instead of learning unreferenced functions. We provide
                   comprehensive empirical evidence showing that these residual
                   networks are easier to optimize, and can gain accuracy from
                   considerably increased depth. On the ImageNet dataset we
                   evaluate residual nets with a depth of up to 152 layers---8x
                   deeper than VGG nets but still having lower complexity. An
                   ensemble of these residual nets achieves 3.57\% error on the
                   ImageNet test set. This result won the 1st place on the
                   ILSVRC 2015 classification task. We also present analysis on
                   CIFAR-10 with 100 and 1000 layers. The depth of
                   representations is of central importance for many visual
                   recognition tasks. Solely due to our extremely deep
                   representations, we obtain a 28\% relative improvement on
                   the COCO object detection dataset. Deep residual nets are
                   foundations of our submissions to ILSVRC \& COCO 2015
                   competitions, where we also won the 1st places on the tasks
                   of ImageNet detection, ImageNet localization, COCO
                   detection, and COCO segmentation.",
  month         =  "10~" # dec,
  year          =  2015,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "1512.03385"
}

@ARTICLE{Ioffe2015-gy,
  title         = "Batch Normalization: Accelerating Deep Network Training by
                   Reducing Internal Covariate Shift",
  author        = "Ioffe, Sergey and Szegedy, Christian",
  abstract      = "Training Deep Neural Networks is complicated by the fact
                   that the distribution of each layer's inputs changes during
                   training, as the parameters of the previous layers change.
                   This slows down the training by requiring lower learning
                   rates and careful parameter initialization, and makes it
                   notoriously hard to train models with saturating
                   nonlinearities. We refer to this phenomenon as internal
                   covariate shift, and address the problem by normalizing
                   layer inputs. Our method draws its strength from making
                   normalization a part of the model architecture and
                   performing the normalization for each training mini-batch.
                   Batch Normalization allows us to use much higher learning
                   rates and be less careful about initialization. It also acts
                   as a regularizer, in some cases eliminating the need for
                   Dropout. Applied to a state-of-the-art image classification
                   model, Batch Normalization achieves the same accuracy with
                   14 times fewer training steps, and beats the original model
                   by a significant margin. Using an ensemble of
                   batch-normalized networks, we improve upon the best
                   published result on ImageNet classification: reaching 4.9\%
                   top-5 validation error (and 4.8\% test error), exceeding the
                   accuracy of human raters.",
  month         =  "11~" # feb,
  year          =  2015,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "1502.03167"
}

@ARTICLE{Glorot2011-bj,
  title     = "Deep Sparse Rectifier Neural Networks",
  author    = "Glorot, X and Bordes, A and Bengio, Y",
  abstract  = "Abstract While logistic sigmoid neurons are more biologically
               plausible than hyperbolic tangent neurons, the latter work
               better for training multi-layer neural networks. This paper
               shows that rectifying neurons are an even better model of
               biological neurons and yield equal or better performance than
               hyperbolic tangent networks in spite of the hard non- linearity
               and non-differentiability at zero, creating sparse
               representations with true zeros, ...",
  journal   = "Aistats",
  publisher = "jmlr.org",
  year      =  2011
}

@ARTICLE{Theis2016-sc,
  title       = "Benchmarking Spike Rate Inference in Population Calcium
                 Imaging",
  author      = "Theis, Lucas and Berens, Philipp and Froudarakis, Emmanouil
                 and Reimer, Jacob and Rom{\'a}n Ros{\'o}n, Miroslav and Baden,
                 Tom and Euler, Thomas and Tolias, Andreas S and Bethge,
                 Matthias",
  affiliation = "Centre for Integrative Neuroscience, University of
                 T{\"u}bingen, 72076 T{\"u}bingen, Germany; Institute of
                 Theoretical Physics, University of T{\"u}bingen, 72076
                 T{\"u}bingen, Germany. Centre for Integrative Neuroscience,
                 University of T{\"u}bingen, 72076 T{\"u}bingen, Germany;
                 Institute of Theoretical Physics, University of T{\"u}bingen,
                 72076 T{\"u}bingen, Germany; Bernstein Center for
                 Computational Neuroscience, University of T{\"u}bingen, 72076
                 T{\"u}bingen, Germany; Institute for Ophthalmic Research,
                 University of T{\"u}bingen, 72076 T{\"u}bingen, Germany;
                 Department of Neuroscience, Baylor College of Medicine,
                 Houston, 77030, USA. Electronic address:
                 philipp.berens@uni-tuebingen.de. Department of Neuroscience,
                 Baylor College of Medicine, Houston, 77030, USA. Department of
                 Neuroscience, Baylor College of Medicine, Houston, 77030, USA.
                 Centre for Integrative Neuroscience, University of
                 T{\"u}bingen, 72076 T{\"u}bingen, Germany; Institute for
                 Ophthalmic Research, University of T{\"u}bingen, 72076
                 T{\"u}bingen, Germany. Centre for Integrative Neuroscience,
                 University of T{\"u}bingen, 72076 T{\"u}bingen, Germany;
                 Bernstein Center for Computational Neuroscience, University of
                 T{\"u}bingen, 72076 T{\"u}bingen, Germany; School of Life
                 Sciences, University of Sussex, Brighton, BN1 9RH, UK. Centre
                 for Integrative Neuroscience, University of T{\"u}bingen,
                 72076 T{\"u}bingen, Germany; Bernstein Center for
                 Computational Neuroscience, University of T{\"u}bingen, 72076
                 T{\"u}bingen, Germany; Institute for Ophthalmic Research,
                 University of T{\"u}bingen, 72076 T{\"u}bingen, Germany.
                 Bernstein Center for Computational Neuroscience, University of
                 T{\"u}bingen, 72076 T{\"u}bingen, Germany; Department of
                 Neuroscience, Baylor College of Medicine, Houston, 77030, USA.
                 Centre for Integrative Neuroscience, University of
                 T{\"u}bingen, 72076 T{\"u}bingen, Germany; Institute of
                 Theoretical Physics, University of T{\"u}bingen, 72076
                 T{\"u}bingen, Germany; Bernstein Center for Computational
                 Neuroscience, University of T{\"u}bingen, 72076 T{\"u}bingen,
                 Germany; Max Planck Institute for Biological Cybernetics,
                 72076 T{\"u}bingen, Germany. Electronic address:
                 matthias.bethge@uni-tuebingen.de.",
  abstract    = "A fundamental challenge in calcium imaging has been to infer
                 spike rates of neurons from the measured noisy fluorescence
                 traces. We systematically evaluate different spike inference
                 algorithms on a large benchmark dataset (>100,000 spikes)
                 recorded from varying neural tissue (V1 and retina) using
                 different calcium indicators (OGB-1 and GCaMP6). In addition,
                 we introduce a new algorithm based on supervised learning in
                 flexible probabilistic models and find that it performs better
                 than other published techniques. Importantly, it outperforms
                 other algorithms even when applied to entirely new datasets
                 for which no simultaneously recorded data is available. Future
                 data acquired in new experimental conditions can be used to
                 further improve the spike prediction accuracy and
                 generalization performance of the model. Finally, we show that
                 comparing algorithms on artificial data is not informative
                 about performance on real data, suggesting that benchmarking
                 different methods with real-world datasets may greatly
                 facilitate future algorithmic developments in neuroscience.",
  journal     = "Neuron",
  volume      =  90,
  number      =  3,
  pages       = "471--482",
  month       =  "4~" # may,
  year        =  2016,
  language    = "en"
}

@ARTICLE{Graves2013-tf,
  title         = "Generating Sequences With Recurrent Neural Networks",
  author        = "Graves, Alex",
  abstract      = "This paper shows how Long Short-term Memory recurrent neural
                   networks can be used to generate complex sequences with
                   long-range structure, simply by predicting one data point at
                   a time. The approach is demonstrated for text (where the
                   data are discrete) and online handwriting (where the data
                   are real-valued). It is then extended to handwriting
                   synthesis by allowing the network to condition its
                   predictions on a text sequence. The resulting system is able
                   to generate highly realistic cursive handwriting in a wide
                   variety of styles.",
  month         =  "4~" # aug,
  year          =  2013,
  archivePrefix = "arXiv",
  primaryClass  = "cs.NE",
  eprint        = "1308.0850"
}
